{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/onos/Desktop/lstm200.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10904/1652721279.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/onos/Downloads/flask_SDN/Flask-SDN/config.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ip_local'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'lstm'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/onos/Desktop/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Downloads/flask_SDN/Flask-SDN/new_venv/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/flask_SDN/Flask-SDN/new_venv/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/flask_SDN/Flask-SDN/new_venv/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/flask_SDN/Flask-SDN/new_venv/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/flask_SDN/Flask-SDN/new_venv/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/flask_SDN/Flask-SDN/new_venv/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/flask_SDN/Flask-SDN/new_venv/lib/python3.7/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/flask_SDN/Flask-SDN/new_venv/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/onos/Desktop/lstm200.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "ip = str(json.load(open('/home/onos/Downloads/flask_SDN/Flask-SDN/config.json'))['ip_local'])\n",
    "file_name = 'lstm'+str(ip).split('.')[-1]\n",
    "dataframe = pd.read_csv('/home/onos/Desktop/' + file_name + '.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiền Xử Lý"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.drop(['src', 'dst'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xu ly mat can bang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "# Separate majority and minority classes\n",
    "df_minority = dataframe[dataframe['label'] ==1]\n",
    "df_majority = dataframe[dataframe['label'] ==0]\n",
    "# Downsample majority class\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                 replace=False,    \n",
    "                                 n_samples= 35000)\n",
    "\n",
    "#Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     \n",
    "                                 n_samples= 35000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f02c4369610>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD1CAYAAACyaJl6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAARSElEQVR4nO3df6yW5X3H8fenIK1Z14H1jDB+DFNZGjQp2jNk6f7oNOWH+wOadI3+UYkxpVsxaZNmEfvHaLUm9Y/WxMSa0MjEpSslto2kwzJCXZpmUTm2FEXrOEMdEKpUUGvMdNDv/jgX89npczgPHDgHPe9Xcue57+91Xfdz3ckJn/Pc9/UcUlVIkia390z0BCRJE88wkCQZBpIkw0CShGEgScIwkCQBUyd6Amfq4osvrvnz50/0NCTpHeWJJ574TVX1Da+/Y8Ng/vz5DAwMTPQ0JOkdJckL3ereJpIkGQaSJMNAkoRhIEnCMJAk0UMYJHlfkseT/DLJ3iRfbfX7kzyXZHfbFrV6ktydZDDJniRXdpxrdZJ9bVvdUf9okifbmLuT5FxcrCSpu16Wlr4JXF1Vrye5APhZkodb299X1YPD+q8AFrTtKuBe4KokFwHrgX6ggCeSbK2qY63PZ4HHgG3AcuBhJEnjYtRPBjXk9XZ4QdtO9Z8grAQeaOMeBaYnmQUsA3ZU1dEWADuA5a3tA1X1aA395woPAKvGcE2SpNPU05fOkkwBngAuBe6pqseS/B1wR5J/AHYC66rqTWA2cKBj+MFWO1X9YJd6t3msAdYAzJs3r5epT7j56/5loqfwrvH81/96oqfwruLP5tn1Tv/57OkBclWdqKpFwBxgcZLLgVuBDwN/DlwE3HLOZvn2PDZUVX9V9ff1/d63qSVJZ+i0VhNV1SvAI8DyqjrcbgW9CfwjsLh1OwTM7Rg2p9VOVZ/TpS5JGie9rCbqSzK97V8IfAL4VbvXT1v5swp4qg3ZCtzQVhUtAV6tqsPAdmBpkhlJZgBLge2t7bUkS9q5bgAeOruXKUk6lV6eGcwCNrXnBu8BtlTVj5L8JEkfEGA38Let/zbgWmAQeAO4EaCqjia5HdjV+t1WVUfb/ueB+4ELGVpF5EoiSRpHo4ZBVe0BruhSv3qE/gWsHaFtI7CxS30AuHy0uUiSzg2/gSxJMgwkSYaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkkQPYZDkfUkeT/LLJHuTfLXVL0nyWJLBJN9LMq3V39uOB1v7/I5z3drqzyZZ1lFf3mqDSdad/cuUJJ1KL58M3gSurqqPAIuA5UmWAHcCd1XVpcAx4KbW/ybgWKvf1fqRZCFwHXAZsBz4VpIpSaYA9wArgIXA9a2vJGmcjBoGNeT1dnhB2wq4Gniw1TcBq9r+ynZMa78mSVp9c1W9WVXPAYPA4rYNVtX+qnoL2Nz6SpLGSU/PDNpv8LuBl4AdwH8Cr1TV8dblIDC77c8GDgC09leBD3bWh40ZqS5JGic9hUFVnaiqRcAchn6T//A5ndUIkqxJMpBk4MiRIxMxBUl6Vzqt1URV9QrwCPAXwPQkU1vTHOBQ2z8EzAVo7X8EvNxZHzZmpHq3999QVf1V1d/X13c6U5cknUIvq4n6kkxv+xcCnwCeYSgUPtW6rQYeavtb2zGt/SdVVa1+XVttdAmwAHgc2AUsaKuTpjH0kHnr2bg4SVJvpo7ehVnAprbq5z3Alqr6UZKngc1Jvgb8Ariv9b8P+Kckg8BRhv5xp6r2JtkCPA0cB9ZW1QmAJDcD24EpwMaq2nvWrlCSNKpRw6Cq9gBXdKnvZ+j5wfD6fwN/M8K57gDu6FLfBmzrYb6SpHPAbyBLkgwDSZJhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiS6CEMksxN8kiSp5PsTfKFVv9KkkNJdrft2o4xtyYZTPJskmUd9eWtNphkXUf9kiSPtfr3kkw72xcqSRpZL58MjgNfqqqFwBJgbZKFre2uqlrUtm0Are064DJgOfCtJFOSTAHuAVYAC4HrO85zZzvXpcAx4KazdH2SpB6MGgZVdbiqft72fws8A8w+xZCVwOaqerOqngMGgcVtG6yq/VX1FrAZWJkkwNXAg238JmDVmV6QJOn0ndYzgyTzgSuAx1rp5iR7kmxMMqPVZgMHOoYdbLWR6h8EXqmq48PqkqRx0nMYJHk/8H3gi1X1GnAv8CFgEXAY+MY5meH/n8OaJANJBo4cOXKu306SJo2ewiDJBQwFwXeq6gcAVfViVZ2oqt8B32boNhDAIWBux/A5rTZS/WVgepKpw+q/p6o2VFV/VfX39fX1MnVJUg96WU0U4D7gmar6Zkd9Vke3TwJPtf2twHVJ3pvkEmAB8DiwC1jQVg5NY+gh89aqKuAR4FNt/GrgobFdliTpdEwdvQsfAz4DPJlkd6t9maHVQIuAAp4HPgdQVXuTbAGeZmgl0tqqOgGQ5GZgOzAF2FhVe9v5bgE2J/ka8AuGwkeSNE5GDYOq+hmQLk3bTjHmDuCOLvVt3cZV1X7evs0kSRpnfgNZkmQYSJIMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSSJHsIgydwkjyR5OsneJF9o9YuS7Eiyr73OaPUkuTvJYJI9Sa7sONfq1n9fktUd9Y8mebKNuTtJzsXFSpK66+WTwXHgS1W1EFgCrE2yEFgH7KyqBcDOdgywAljQtjXAvTAUHsB64CpgMbD+ZIC0Pp/tGLd87JcmSerVqGFQVYer6udt/7fAM8BsYCWwqXXbBKxq+yuBB2rIo8D0JLOAZcCOqjpaVceAHcDy1vaBqnq0qgp4oONckqRxcFrPDJLMB64AHgNmVtXh1vRrYGbbnw0c6Bh2sNVOVT/YpS5JGic9h0GS9wPfB75YVa91trXf6Ossz63bHNYkGUgycOTIkXP9dpI0afQUBkkuYCgIvlNVP2jlF9stHtrrS61+CJjbMXxOq52qPqdL/fdU1Yaq6q+q/r6+vl6mLknqQS+riQLcBzxTVd/saNoKnFwRtBp4qKN+Q1tVtAR4td1O2g4sTTKjPTheCmxvba8lWdLe64aOc0mSxsHUHvp8DPgM8GSS3a32ZeDrwJYkNwEvAJ9ubduAa4FB4A3gRoCqOprkdmBX63dbVR1t+58H7gcuBB5umyRpnIwaBlX1M2Ckdf/XdOlfwNoRzrUR2NilPgBcPtpcJEnnht9AliQZBpIkw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJNFDGCTZmOSlJE911L6S5FCS3W27tqPt1iSDSZ5NsqyjvrzVBpOs66hfkuSxVv9ekmln8wIlSaPr5ZPB/cDyLvW7qmpR27YBJFkIXAdc1sZ8K8mUJFOAe4AVwELg+tYX4M52rkuBY8BNY7kgSdLpGzUMquqnwNEez7cS2FxVb1bVc8AgsLhtg1W1v6reAjYDK5MEuBp4sI3fBKw6zWuQJI3RWJ4Z3JxkT7uNNKPVZgMHOvocbLWR6h8EXqmq48PqkqRxdKZhcC/wIWARcBj4xlmb0SkkWZNkIMnAkSNHxuMtJWlSOKMwqKoXq+pEVf0O+DZDt4EADgFzO7rOabWR6i8D05NMHVYf6X03VFV/VfX39fWdydQlSV2cURgkmdVx+Eng5EqjrcB1Sd6b5BJgAfA4sAtY0FYOTWPoIfPWqirgEeBTbfxq4KEzmZMk6cxNHa1Dku8CHwcuTnIQWA98PMkioIDngc8BVNXeJFuAp4HjwNqqOtHOczOwHZgCbKyqve0tbgE2J/ka8AvgvrN2dZKknowaBlV1fZfyiP9gV9UdwB1d6tuAbV3q+3n7NpMkaQL4DWRJkmEgSTIMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiR6CIMkG5O8lOSpjtpFSXYk2ddeZ7R6ktydZDDJniRXdoxZ3frvS7K6o/7RJE+2MXcnydm+SEnSqfXyyeB+YPmw2jpgZ1UtAHa2Y4AVwIK2rQHuhaHwANYDVwGLgfUnA6T1+WzHuOHvJUk6x0YNg6r6KXB0WHklsKntbwJWddQfqCGPAtOTzAKWATuq6mhVHQN2AMtb2weq6tGqKuCBjnNJksbJmT4zmFlVh9v+r4GZbX82cKCj38FWO1X9YJe6JGkcjfkBcvuNvs7CXEaVZE2SgSQDR44cGY+3lKRJ4UzD4MV2i4f2+lKrHwLmdvSb02qnqs/pUu+qqjZUVX9V9ff19Z3h1CVJw51pGGwFTq4IWg081FG/oa0qWgK82m4nbQeWJpnRHhwvBba3tteSLGmriG7oOJckaZxMHa1Dku8CHwcuTnKQoVVBXwe2JLkJeAH4dOu+DbgWGATeAG4EqKqjSW4HdrV+t1XVyYfSn2doxdKFwMNtkySNo1HDoKquH6Hpmi59C1g7wnk2Ahu71AeAy0ebhyTp3PEbyJIkw0CSZBhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiTGGAZJnk/yZJLdSQZa7aIkO5Lsa68zWj1J7k4ymGRPkis7zrO69d+XZPXYLkmSdLrOxieDv6qqRVXV347XATuragGwsx0DrAAWtG0NcC8MhQewHrgKWAysPxkgkqTxcS5uE60ENrX9TcCqjvoDNeRRYHqSWcAyYEdVHa2qY8AOYPk5mJckaQRjDYMC/jXJE0nWtNrMqjrc9n8NzGz7s4EDHWMPttpIdUnSOJk6xvF/WVWHkvwxsCPJrzobq6qS1Bjf4/+0wFkDMG/evLN1Wkma9Mb0yaCqDrXXl4AfMnTP/8V2+4f2+lLrfgiY2zF8TquNVO/2fhuqqr+q+vv6+sYydUlShzMOgyR/kOQPT+4DS4GngK3AyRVBq4GH2v5W4Ia2qmgJ8Gq7nbQdWJpkRntwvLTVJEnjZCy3iWYCP0xy8jz/XFU/TrIL2JLkJuAF4NOt/zbgWmAQeAO4EaCqjia5HdjV+t1WVUfHMC9J0mk64zCoqv3AR7rUXwau6VIvYO0I59oIbDzTuUiSxsZvIEuSDANJkmEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJI4j8IgyfIkzyYZTLJuoucjSZPJeREGSaYA9wArgIXA9UkWTuysJGnyOC/CAFgMDFbV/qp6C9gMrJzgOUnSpDF1oifQzAYOdBwfBK4a3inJGmBNO3w9ybPjMLfJ4GLgNxM9idHkzomegSaIP59n1592K54vYdCTqtoAbJjoebzbJBmoqv6JnofUjT+f4+N8uU10CJjbcTyn1SRJ4+B8CYNdwIIklySZBlwHbJ3gOUnSpHFe3CaqquNJbga2A1OAjVW1d4KnNZl4603nM38+x0GqaqLnIEmaYOfLbSJJ0gQyDCRJhoEk6Tx5gKzxleTDDH3De3YrHQK2VtUzEzcrSRPJTwaTTJJbGPpzHwEeb1uA7/oHAnU+S3LjRM/h3czVRJNMkv8ALquq/xlWnwbsraoFEzMz6dSS/FdVzZvoebxbeZto8vkd8CfAC8Pqs1qbNGGS7BmpCZg5nnOZbAyDyeeLwM4k+3j7jwPOAy4Fbp6wWUlDZgLLgGPD6gH+ffynM3kYBpNMVf04yZ8x9GfDOx8g76qqExM3MwmAHwHvr6rdwxuS/Nv4T2fy8JmBJMnVRJIkw0CShGEgScIwkCRhGEiSgP8F8AEz/PjYc1MAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_up_down_sampled = pd.concat([df_majority_downsampled, df_minority_upsampled])\n",
    "df_up_down_sampled['label'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Min Max scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_up_down_sampled['label']\n",
    "X = df_up_down_sampled.drop(columns='label')\n",
    "y_set = y.values\n",
    "X_set = X.values.astype('float32')\n",
    "\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_set = scaler.fit_transform(X_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.save']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import sklearn.external.joblib as extjoblib\n",
    "import joblib\n",
    "scaler_filename = \"scaler.save\"\n",
    "joblib.dump(scaler, scaler_filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "train_X, test_X, train_y, test_y = train_test_split(X_set, y_set, test_size = 0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thay đổi shape của tập X\n",
    "time_steps = 1\n",
    "input_train_lstm = train_X.reshape( train_X.shape[0], time_steps, train_X.shape[1] )\n",
    "\n",
    "input_test_lstm = test_X.reshape(   test_X.shape[0], time_steps, test_X.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_3 (LSTM)               (None, 1, 64)             17664     \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 1, 128)            98816     \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 165,953\n",
      "Trainable params: 165,953\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/100\n",
      "1527/1532 [============================>.] - ETA: 0s - loss: 0.6671 - accuracy: 0.8240\n",
      "Epoch 1: accuracy improved from -inf to 0.82435, saving model to best_model_lstm_30_610.20.0.200.hdf5\n",
      "1532/1532 [==============================] - 12s 6ms/step - loss: 0.6657 - accuracy: 0.8243\n",
      "Epoch 2/100\n",
      "1527/1532 [============================>.] - ETA: 0s - loss: 0.2998 - accuracy: 0.9125\n",
      "Epoch 2: accuracy improved from 0.82435 to 0.91255, saving model to best_model_lstm_30_610.20.0.200.hdf5\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.2994 - accuracy: 0.9126\n",
      "Epoch 3/100\n",
      "1529/1532 [============================>.] - ETA: 0s - loss: 0.1465 - accuracy: 0.9422\n",
      "Epoch 3: accuracy improved from 0.91255 to 0.94222, saving model to best_model_lstm_30_610.20.0.200.hdf5\n",
      "1532/1532 [==============================] - 10s 6ms/step - loss: 0.1465 - accuracy: 0.9422\n",
      "Epoch 4/100\n",
      "1525/1532 [============================>.] - ETA: 0s - loss: 0.4280 - accuracy: 0.9418\n",
      "Epoch 4: accuracy did not improve from 0.94222\n",
      "1532/1532 [==============================] - 10s 6ms/step - loss: 0.4265 - accuracy: 0.9419\n",
      "Epoch 5/100\n",
      "1528/1532 [============================>.] - ETA: 0s - loss: 0.5058 - accuracy: 0.9426\n",
      "Epoch 5: accuracy did not improve from 0.94222\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.5098 - accuracy: 0.9421\n",
      "Epoch 6/100\n",
      "1527/1532 [============================>.] - ETA: 0s - loss: 0.2932 - accuracy: 0.9366\n",
      "Epoch 6: accuracy did not improve from 0.94222\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.2926 - accuracy: 0.9367\n",
      "Epoch 7/100\n",
      "1531/1532 [============================>.] - ETA: 0s - loss: 0.0759 - accuracy: 0.9692\n",
      "Epoch 7: accuracy improved from 0.94222 to 0.96922, saving model to best_model_lstm_30_610.20.0.200.hdf5\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.0759 - accuracy: 0.9692\n",
      "Epoch 8/100\n",
      "1524/1532 [============================>.] - ETA: 0s - loss: 0.6299 - accuracy: 0.9310\n",
      "Epoch 8: accuracy did not improve from 0.96922\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.6272 - accuracy: 0.9312\n",
      "Epoch 9/100\n",
      "1527/1532 [============================>.] - ETA: 0s - loss: 0.5406 - accuracy: 0.9374\n",
      "Epoch 9: accuracy did not improve from 0.96922\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.5393 - accuracy: 0.9374\n",
      "Epoch 10/100\n",
      "1523/1532 [============================>.] - ETA: 0s - loss: 0.0647 - accuracy: 0.9759\n",
      "Epoch 10: accuracy improved from 0.96922 to 0.97592, saving model to best_model_lstm_30_610.20.0.200.hdf5\n",
      "1532/1532 [==============================] - 8s 5ms/step - loss: 0.0646 - accuracy: 0.9759\n",
      "Epoch 11/100\n",
      "1527/1532 [============================>.] - ETA: 0s - loss: 0.0910 - accuracy: 0.9776\n",
      "Epoch 11: accuracy improved from 0.97592 to 0.97761, saving model to best_model_lstm_30_610.20.0.200.hdf5\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.0909 - accuracy: 0.9776\n",
      "Epoch 12/100\n",
      "1523/1532 [============================>.] - ETA: 0s - loss: 0.0723 - accuracy: 0.9834\n",
      "Epoch 12: accuracy improved from 0.97761 to 0.98345, saving model to best_model_lstm_30_610.20.0.200.hdf5\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.0723 - accuracy: 0.9834\n",
      "Epoch 13/100\n",
      "1522/1532 [============================>.] - ETA: 0s - loss: 0.5261 - accuracy: 0.9277\n",
      "Epoch 13: accuracy did not improve from 0.98345\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.5242 - accuracy: 0.9277\n",
      "Epoch 14/100\n",
      "1531/1532 [============================>.] - ETA: 0s - loss: 0.1030 - accuracy: 0.9627\n",
      "Epoch 14: accuracy did not improve from 0.98345\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.1030 - accuracy: 0.9627\n",
      "Epoch 15/100\n",
      "1530/1532 [============================>.] - ETA: 0s - loss: 0.1669 - accuracy: 0.9717\n",
      "Epoch 15: accuracy did not improve from 0.98345\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.1668 - accuracy: 0.9717\n",
      "Epoch 16/100\n",
      "1526/1532 [============================>.] - ETA: 0s - loss: 0.0545 - accuracy: 0.9809\n",
      "Epoch 16: accuracy did not improve from 0.98345\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.0544 - accuracy: 0.9809\n",
      "Epoch 17/100\n",
      "1529/1532 [============================>.] - ETA: 0s - loss: 0.1405 - accuracy: 0.9779\n",
      "Epoch 17: accuracy did not improve from 0.98345\n",
      "1532/1532 [==============================] - 8s 5ms/step - loss: 0.1403 - accuracy: 0.9779\n",
      "Epoch 18/100\n",
      "1522/1532 [============================>.] - ETA: 0s - loss: 0.1157 - accuracy: 0.9816\n",
      "Epoch 18: accuracy did not improve from 0.98345\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.1152 - accuracy: 0.9816\n",
      "Epoch 19/100\n",
      "1528/1532 [============================>.] - ETA: 0s - loss: 0.0403 - accuracy: 0.9895\n",
      "Epoch 19: accuracy improved from 0.98345 to 0.98949, saving model to best_model_lstm_30_610.20.0.200.hdf5\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.0405 - accuracy: 0.9895\n",
      "Epoch 20/100\n",
      "1530/1532 [============================>.] - ETA: 0s - loss: 0.3452 - accuracy: 0.9664\n",
      "Epoch 20: accuracy did not improve from 0.98949\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.3449 - accuracy: 0.9664\n",
      "Epoch 21/100\n",
      "1531/1532 [============================>.] - ETA: 0s - loss: 0.1890 - accuracy: 0.9677\n",
      "Epoch 21: accuracy did not improve from 0.98949\n",
      "1532/1532 [==============================] - 8s 5ms/step - loss: 0.1889 - accuracy: 0.9678\n",
      "Epoch 22/100\n",
      "1525/1532 [============================>.] - ETA: 0s - loss: 0.0430 - accuracy: 0.9858\n",
      "Epoch 22: accuracy did not improve from 0.98949\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.0431 - accuracy: 0.9858\n",
      "Epoch 23/100\n",
      "1530/1532 [============================>.] - ETA: 0s - loss: 0.0503 - accuracy: 0.9892\n",
      "Epoch 23: accuracy did not improve from 0.98949\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.0503 - accuracy: 0.9892\n",
      "Epoch 24/100\n",
      "1531/1532 [============================>.] - ETA: 0s - loss: 0.0285 - accuracy: 0.9922\n",
      "Epoch 24: accuracy improved from 0.98949 to 0.99214, saving model to best_model_lstm_30_610.20.0.200.hdf5\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.0285 - accuracy: 0.9921\n",
      "Epoch 25/100\n",
      "1529/1532 [============================>.] - ETA: 0s - loss: 0.0930 - accuracy: 0.9877\n",
      "Epoch 25: accuracy did not improve from 0.99214\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.0929 - accuracy: 0.9877\n",
      "Epoch 26/100\n",
      "1528/1532 [============================>.] - ETA: 0s - loss: 0.5777 - accuracy: 0.9485\n",
      "Epoch 26: accuracy did not improve from 0.99214\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.5765 - accuracy: 0.9486\n",
      "Epoch 27/100\n",
      "1527/1532 [============================>.] - ETA: 0s - loss: 0.7504 - accuracy: 0.9382\n",
      "Epoch 27: accuracy did not improve from 0.99214\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.7680 - accuracy: 0.9370\n",
      "Epoch 28/100\n",
      "1532/1532 [==============================] - ETA: 0s - loss: 0.7078 - accuracy: 0.9381\n",
      "Epoch 28: accuracy did not improve from 0.99214\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.7078 - accuracy: 0.9381\n",
      "Epoch 29/100\n",
      "1526/1532 [============================>.] - ETA: 0s - loss: 0.0423 - accuracy: 0.9898\n",
      "Epoch 29: accuracy did not improve from 0.99214\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.0422 - accuracy: 0.9898\n",
      "Epoch 30/100\n",
      "1523/1532 [============================>.] - ETA: 0s - loss: 0.3970 - accuracy: 0.9580\n",
      "Epoch 30: accuracy did not improve from 0.99214\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.3970 - accuracy: 0.9579\n",
      "Epoch 31/100\n",
      "1532/1532 [==============================] - ETA: 0s - loss: 0.0765 - accuracy: 0.9803\n",
      "Epoch 31: accuracy did not improve from 0.99214\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.0765 - accuracy: 0.9803\n",
      "Epoch 32/100\n",
      "1527/1532 [============================>.] - ETA: 0s - loss: 0.0393 - accuracy: 0.9888\n",
      "Epoch 32: accuracy did not improve from 0.99214\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.0392 - accuracy: 0.9888\n",
      "Epoch 33/100\n",
      "1532/1532 [==============================] - ETA: 0s - loss: 0.0971 - accuracy: 0.9826\n",
      "Epoch 33: accuracy did not improve from 0.99214\n",
      "1532/1532 [==============================] - 8s 6ms/step - loss: 0.0971 - accuracy: 0.9826\n",
      "Epoch 34/100\n",
      "1526/1532 [============================>.] - ETA: 0s - loss: 0.0990 - accuracy: 0.9841\n",
      "Epoch 34: accuracy did not improve from 0.99214\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.0988 - accuracy: 0.9842\n",
      "Epoch 35/100\n",
      "1523/1532 [============================>.] - ETA: 0s - loss: 0.0697 - accuracy: 0.9876\n",
      "Epoch 35: accuracy did not improve from 0.99214\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.0694 - accuracy: 0.9876\n",
      "Epoch 36/100\n",
      "1524/1532 [============================>.] - ETA: 0s - loss: 0.5134 - accuracy: 0.9548\n",
      "Epoch 36: accuracy did not improve from 0.99214\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.5289 - accuracy: 0.9537\n",
      "Epoch 37/100\n",
      "1531/1532 [============================>.] - ETA: 0s - loss: 0.6173 - accuracy: 0.9354\n",
      "Epoch 37: accuracy did not improve from 0.99214\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.6175 - accuracy: 0.9354\n",
      "Epoch 38/100\n",
      "1525/1532 [============================>.] - ETA: 0s - loss: 1.1001 - accuracy: 0.9006\n",
      "Epoch 38: accuracy did not improve from 0.99214\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 1.0965 - accuracy: 0.9009\n",
      "Epoch 39/100\n",
      "1529/1532 [============================>.] - ETA: 0s - loss: 0.2619 - accuracy: 0.9605\n",
      "Epoch 39: accuracy did not improve from 0.99214\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.2616 - accuracy: 0.9606\n",
      "Epoch 40/100\n",
      "1528/1532 [============================>.] - ETA: 0s - loss: 0.0476 - accuracy: 0.9842\n",
      "Epoch 40: accuracy did not improve from 0.99214\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.0476 - accuracy: 0.9841\n",
      "Epoch 41/100\n",
      "1532/1532 [==============================] - ETA: 0s - loss: 0.0308 - accuracy: 0.9914\n",
      "Epoch 41: accuracy did not improve from 0.99214\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.0308 - accuracy: 0.9914\n",
      "Epoch 42/100\n",
      "1532/1532 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 0.9931\n",
      "Epoch 42: accuracy improved from 0.99214 to 0.99310, saving model to best_model_lstm_30_610.20.0.200.hdf5\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.0229 - accuracy: 0.9931\n",
      "Epoch 43/100\n",
      "1529/1532 [============================>.] - ETA: 0s - loss: 0.1981 - accuracy: 0.9727\n",
      "Epoch 43: accuracy did not improve from 0.99310\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.1979 - accuracy: 0.9727\n",
      "Epoch 44/100\n",
      "1528/1532 [============================>.] - ETA: 0s - loss: 0.0345 - accuracy: 0.9921\n",
      "Epoch 44: accuracy did not improve from 0.99310\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.0345 - accuracy: 0.9921\n",
      "Epoch 45/100\n",
      "1528/1532 [============================>.] - ETA: 0s - loss: 0.0231 - accuracy: 0.9931\n",
      "Epoch 45: accuracy did not improve from 0.99310\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.0231 - accuracy: 0.9931\n",
      "Epoch 46/100\n",
      "1528/1532 [============================>.] - ETA: 0s - loss: 0.2526 - accuracy: 0.9755\n",
      "Epoch 46: accuracy did not improve from 0.99310\n",
      "1532/1532 [==============================] - 8s 5ms/step - loss: 0.2547 - accuracy: 0.9753\n",
      "Epoch 47/100\n",
      "1528/1532 [============================>.] - ETA: 0s - loss: 0.1654 - accuracy: 0.9812\n",
      "Epoch 47: accuracy did not improve from 0.99310\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.1676 - accuracy: 0.9810\n",
      "Epoch 48/100\n",
      "1527/1532 [============================>.] - ETA: 0s - loss: 0.4768 - accuracy: 0.9595\n",
      "Epoch 48: accuracy did not improve from 0.99310\n",
      "1532/1532 [==============================] - 8s 5ms/step - loss: 0.4771 - accuracy: 0.9595\n",
      "Epoch 49/100\n",
      "1531/1532 [============================>.] - ETA: 0s - loss: 0.2822 - accuracy: 0.9726\n",
      "Epoch 49: accuracy did not improve from 0.99310\n",
      "1532/1532 [==============================] - 8s 6ms/step - loss: 0.2822 - accuracy: 0.9726\n",
      "Epoch 50/100\n",
      "1530/1532 [============================>.] - ETA: 0s - loss: 0.0651 - accuracy: 0.9876\n",
      "Epoch 50: accuracy did not improve from 0.99310\n",
      "1532/1532 [==============================] - 8s 6ms/step - loss: 0.0651 - accuracy: 0.9876\n",
      "Epoch 51/100\n",
      "1530/1532 [============================>.] - ETA: 0s - loss: 0.2185 - accuracy: 0.9766\n",
      "Epoch 51: accuracy did not improve from 0.99310\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.2183 - accuracy: 0.9766\n",
      "Epoch 52/100\n",
      "1526/1532 [============================>.] - ETA: 0s - loss: 0.1282 - accuracy: 0.9807\n",
      "Epoch 52: accuracy did not improve from 0.99310\n",
      "1532/1532 [==============================] - 8s 5ms/step - loss: 0.1278 - accuracy: 0.9808\n",
      "Epoch 53/100\n",
      "1527/1532 [============================>.] - ETA: 0s - loss: 0.0167 - accuracy: 0.9932\n",
      "Epoch 53: accuracy improved from 0.99310 to 0.99324, saving model to best_model_lstm_30_610.20.0.200.hdf5\n",
      "1532/1532 [==============================] - 8s 5ms/step - loss: 0.0166 - accuracy: 0.9932\n",
      "Epoch 54/100\n",
      "1530/1532 [============================>.] - ETA: 0s - loss: 0.0470 - accuracy: 0.9912\n",
      "Epoch 54: accuracy did not improve from 0.99324\n",
      "1532/1532 [==============================] - 8s 5ms/step - loss: 0.0470 - accuracy: 0.9912\n",
      "Epoch 55/100\n",
      "1521/1532 [============================>.] - ETA: 0s - loss: 0.4108 - accuracy: 0.9636\n",
      "Epoch 55: accuracy did not improve from 0.99324\n",
      "1532/1532 [==============================] - 8s 5ms/step - loss: 0.4081 - accuracy: 0.9638\n",
      "Epoch 56/100\n",
      "1524/1532 [============================>.] - ETA: 0s - loss: 0.0291 - accuracy: 0.9933\n",
      "Epoch 56: accuracy did not improve from 0.99324\n",
      "1532/1532 [==============================] - 8s 5ms/step - loss: 0.0290 - accuracy: 0.9932\n",
      "Epoch 57/100\n",
      "1520/1532 [============================>.] - ETA: 0s - loss: 1.1397 - accuracy: 0.9114\n",
      "Epoch 57: accuracy did not improve from 0.99324\n",
      "1532/1532 [==============================] - 8s 5ms/step - loss: 1.1434 - accuracy: 0.9112\n",
      "Epoch 58/100\n",
      "1531/1532 [============================>.] - ETA: 0s - loss: 0.5168 - accuracy: 0.9577\n",
      "Epoch 58: accuracy did not improve from 0.99324\n",
      "1532/1532 [==============================] - 8s 5ms/step - loss: 0.5168 - accuracy: 0.9577\n",
      "Epoch 59/100\n",
      "1529/1532 [============================>.] - ETA: 0s - loss: 0.2765 - accuracy: 0.9715\n",
      "Epoch 59: accuracy did not improve from 0.99324\n",
      "1532/1532 [==============================] - 7s 5ms/step - loss: 0.2762 - accuracy: 0.9716\n",
      "Epoch 60/100\n",
      "1523/1532 [============================>.] - ETA: 0s - loss: 1.3689 - accuracy: 0.8941\n",
      "Epoch 60: accuracy did not improve from 0.99324\n",
      "1532/1532 [==============================] - 7s 5ms/step - loss: 1.3670 - accuracy: 0.8941\n",
      "Epoch 61/100\n",
      "1524/1532 [============================>.] - ETA: 0s - loss: 0.2709 - accuracy: 0.9741\n",
      "Epoch 61: accuracy did not improve from 0.99324\n",
      "1532/1532 [==============================] - 7s 5ms/step - loss: 0.2698 - accuracy: 0.9741\n",
      "Epoch 62/100\n",
      "1532/1532 [==============================] - ETA: 0s - loss: 2.2688 - accuracy: 0.8327\n",
      "Epoch 62: accuracy did not improve from 0.99324\n",
      "1532/1532 [==============================] - 7s 5ms/step - loss: 2.2688 - accuracy: 0.8327\n",
      "Epoch 63/100\n",
      "1529/1532 [============================>.] - ETA: 0s - loss: 0.2489 - accuracy: 0.9739\n",
      "Epoch 63: accuracy did not improve from 0.99324\n",
      "1532/1532 [==============================] - 7s 5ms/step - loss: 0.2486 - accuracy: 0.9740\n",
      "Epoch 64/100\n",
      "1528/1532 [============================>.] - ETA: 0s - loss: 0.0693 - accuracy: 0.9884\n",
      "Epoch 64: accuracy did not improve from 0.99324\n",
      "1532/1532 [==============================] - 7s 5ms/step - loss: 0.0692 - accuracy: 0.9884\n",
      "Epoch 65/100\n",
      "1531/1532 [============================>.] - ETA: 0s - loss: 0.0329 - accuracy: 0.9933\n",
      "Epoch 65: accuracy improved from 0.99324 to 0.99331, saving model to best_model_lstm_30_610.20.0.200.hdf5\n",
      "1532/1532 [==============================] - 7s 5ms/step - loss: 0.0329 - accuracy: 0.9933\n",
      "Epoch 66/100\n",
      "1521/1532 [============================>.] - ETA: 0s - loss: 0.0170 - accuracy: 0.9954\n",
      "Epoch 66: accuracy improved from 0.99331 to 0.99539, saving model to best_model_lstm_30_610.20.0.200.hdf5\n",
      "1532/1532 [==============================] - 7s 5ms/step - loss: 0.0172 - accuracy: 0.9954\n",
      "Epoch 67/100\n",
      "1522/1532 [============================>.] - ETA: 0s - loss: 0.1757 - accuracy: 0.9812\n",
      "Epoch 67: accuracy did not improve from 0.99539\n",
      "1532/1532 [==============================] - 7s 5ms/step - loss: 0.1787 - accuracy: 0.9810\n",
      "Epoch 68/100\n",
      "1525/1532 [============================>.] - ETA: 0s - loss: 1.2561 - accuracy: 0.9072\n",
      "Epoch 68: accuracy did not improve from 0.99539\n",
      "1532/1532 [==============================] - 7s 5ms/step - loss: 1.2521 - accuracy: 0.9075\n",
      "Epoch 69/100\n",
      "1525/1532 [============================>.] - ETA: 0s - loss: 0.0500 - accuracy: 0.9887\n",
      "Epoch 69: accuracy did not improve from 0.99539\n",
      "1532/1532 [==============================] - 7s 5ms/step - loss: 0.0498 - accuracy: 0.9888\n",
      "Epoch 70/100\n",
      "1524/1532 [============================>.] - ETA: 0s - loss: 0.0490 - accuracy: 0.9911\n",
      "Epoch 70: accuracy did not improve from 0.99539\n",
      "1532/1532 [==============================] - 7s 5ms/step - loss: 0.0488 - accuracy: 0.9911\n",
      "Epoch 71/100\n",
      "1525/1532 [============================>.] - ETA: 0s - loss: 0.0909 - accuracy: 0.9886\n",
      "Epoch 71: accuracy did not improve from 0.99539\n",
      "1532/1532 [==============================] - 7s 5ms/step - loss: 0.0906 - accuracy: 0.9887\n",
      "Epoch 72/100\n",
      "1530/1532 [============================>.] - ETA: 0s - loss: 0.1825 - accuracy: 0.9789\n",
      "Epoch 72: accuracy did not improve from 0.99539\n",
      "1532/1532 [==============================] - 7s 5ms/step - loss: 0.1823 - accuracy: 0.9789\n",
      "Epoch 73/100\n",
      "1527/1532 [============================>.] - ETA: 0s - loss: 0.0625 - accuracy: 0.9819\n",
      "Epoch 73: accuracy did not improve from 0.99539\n",
      "1532/1532 [==============================] - 7s 5ms/step - loss: 0.0627 - accuracy: 0.9819\n",
      "Epoch 74/100\n",
      "1526/1532 [============================>.] - ETA: 0s - loss: 0.0250 - accuracy: 0.9921\n",
      "Epoch 74: accuracy did not improve from 0.99539\n",
      "1532/1532 [==============================] - 7s 5ms/step - loss: 0.0251 - accuracy: 0.9921\n",
      "Epoch 75/100\n",
      "1525/1532 [============================>.] - ETA: 0s - loss: 0.0719 - accuracy: 0.9849\n",
      "Epoch 75: accuracy did not improve from 0.99539\n",
      "1532/1532 [==============================] - 7s 5ms/step - loss: 0.0760 - accuracy: 0.9846\n",
      "Epoch 76/100\n",
      "1530/1532 [============================>.] - ETA: 0s - loss: 0.0447 - accuracy: 0.9912\n",
      "Epoch 76: accuracy did not improve from 0.99539\n",
      "1532/1532 [==============================] - 7s 5ms/step - loss: 0.0447 - accuracy: 0.9912\n",
      "Epoch 77/100\n",
      "1527/1532 [============================>.] - ETA: 0s - loss: 0.2103 - accuracy: 0.9803\n",
      "Epoch 77: accuracy did not improve from 0.99539\n",
      "1532/1532 [==============================] - 7s 5ms/step - loss: 0.2098 - accuracy: 0.9803\n",
      "Epoch 78/100\n",
      "1522/1532 [============================>.] - ETA: 0s - loss: 0.0241 - accuracy: 0.9945\n",
      "Epoch 78: accuracy did not improve from 0.99539\n",
      "1532/1532 [==============================] - 7s 5ms/step - loss: 0.0240 - accuracy: 0.9945\n",
      "Epoch 79/100\n",
      "1530/1532 [============================>.] - ETA: 0s - loss: 0.7689 - accuracy: 0.9432\n",
      "Epoch 79: accuracy did not improve from 0.99539\n",
      "1532/1532 [==============================] - 7s 5ms/step - loss: 0.7686 - accuracy: 0.9432\n",
      "Epoch 80/100\n",
      "1530/1532 [============================>.] - ETA: 0s - loss: 2.4601 - accuracy: 0.8320\n",
      "Epoch 80: accuracy did not improve from 0.99539\n",
      "1532/1532 [==============================] - 7s 5ms/step - loss: 2.4587 - accuracy: 0.8321\n",
      "Epoch 81/100\n",
      "1523/1532 [============================>.] - ETA: 0s - loss: 0.5748 - accuracy: 0.9548\n",
      "Epoch 81: accuracy did not improve from 0.99539\n",
      "1532/1532 [==============================] - 7s 5ms/step - loss: 0.5720 - accuracy: 0.9550\n",
      "Epoch 82/100\n",
      "1529/1532 [============================>.] - ETA: 0s - loss: 0.5314 - accuracy: 0.9590\n",
      "Epoch 82: accuracy did not improve from 0.99539\n",
      "1532/1532 [==============================] - 7s 5ms/step - loss: 0.5324 - accuracy: 0.9590\n",
      "Epoch 83/100\n",
      "1531/1532 [============================>.] - ETA: 0s - loss: 0.8974 - accuracy: 0.9359\n",
      "Epoch 83: accuracy did not improve from 0.99539\n",
      "1532/1532 [==============================] - 7s 5ms/step - loss: 0.8973 - accuracy: 0.9359\n",
      "Epoch 84/100\n",
      "1524/1532 [============================>.] - ETA: 0s - loss: 0.4356 - accuracy: 0.9659\n",
      "Epoch 84: accuracy did not improve from 0.99539\n",
      "1532/1532 [==============================] - 7s 5ms/step - loss: 0.4354 - accuracy: 0.9660\n",
      "Epoch 85/100\n",
      "1529/1532 [============================>.] - ETA: 0s - loss: 0.6505 - accuracy: 0.9505\n",
      "Epoch 85: accuracy did not improve from 0.99539\n",
      "1532/1532 [==============================] - 7s 5ms/step - loss: 0.6495 - accuracy: 0.9506\n",
      "Epoch 86/100\n",
      "1522/1532 [============================>.] - ETA: 0s - loss: 0.3392 - accuracy: 0.9720\n",
      "Epoch 86: accuracy did not improve from 0.99539\n",
      "1532/1532 [==============================] - 7s 5ms/step - loss: 0.3382 - accuracy: 0.9720\n",
      "Epoch 87/100\n",
      "1530/1532 [============================>.] - ETA: 0s - loss: 0.2243 - accuracy: 0.9793\n",
      "Epoch 87: accuracy did not improve from 0.99539\n",
      "1532/1532 [==============================] - 7s 5ms/step - loss: 0.2241 - accuracy: 0.9793\n",
      "Epoch 88/100\n",
      "1529/1532 [============================>.] - ETA: 0s - loss: 0.1308 - accuracy: 0.9865\n",
      "Epoch 88: accuracy did not improve from 0.99539\n",
      "1532/1532 [==============================] - 7s 5ms/step - loss: 0.1318 - accuracy: 0.9864\n",
      "Epoch 89/100\n",
      "1526/1532 [============================>.] - ETA: 0s - loss: 0.8673 - accuracy: 0.9374\n",
      "Epoch 89: accuracy did not improve from 0.99539\n",
      "1532/1532 [==============================] - 8s 5ms/step - loss: 0.8737 - accuracy: 0.9370\n",
      "Epoch 90/100\n",
      "1530/1532 [============================>.] - ETA: 0s - loss: 0.8967 - accuracy: 0.9360\n",
      "Epoch 90: accuracy did not improve from 0.99539\n",
      "1532/1532 [==============================] - 8s 5ms/step - loss: 0.8966 - accuracy: 0.9360\n",
      "Epoch 91/100\n",
      "1531/1532 [============================>.] - ETA: 0s - loss: 0.5487 - accuracy: 0.9580\n",
      "Epoch 91: accuracy did not improve from 0.99539\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.5486 - accuracy: 0.9580\n",
      "Epoch 92/100\n",
      "1523/1532 [============================>.] - ETA: 0s - loss: 0.6197 - accuracy: 0.9534\n",
      "Epoch 92: accuracy did not improve from 0.99539\n",
      "1532/1532 [==============================] - 8s 5ms/step - loss: 0.6203 - accuracy: 0.9533\n",
      "Epoch 93/100\n",
      "1527/1532 [============================>.] - ETA: 0s - loss: 0.2236 - accuracy: 0.9723\n",
      "Epoch 93: accuracy did not improve from 0.99539\n",
      "1532/1532 [==============================] - 8s 6ms/step - loss: 0.2230 - accuracy: 0.9724\n",
      "Epoch 94/100\n",
      "1526/1532 [============================>.] - ETA: 0s - loss: 0.0642 - accuracy: 0.9889\n",
      "Epoch 94: accuracy did not improve from 0.99539\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.0640 - accuracy: 0.9889\n",
      "Epoch 95/100\n",
      "1531/1532 [============================>.] - ETA: 0s - loss: 0.0533 - accuracy: 0.9917\n",
      "Epoch 95: accuracy did not improve from 0.99539\n",
      "1532/1532 [==============================] - 9s 6ms/step - loss: 0.0533 - accuracy: 0.9917\n",
      "Epoch 96/100\n",
      "1529/1532 [============================>.] - ETA: 0s - loss: 0.0762 - accuracy: 0.9901\n",
      "Epoch 96: accuracy did not improve from 0.99539\n",
      "1532/1532 [==============================] - 8s 5ms/step - loss: 0.0761 - accuracy: 0.9901\n",
      "Epoch 97/100\n",
      "1524/1532 [============================>.] - ETA: 0s - loss: 0.1819 - accuracy: 0.9838\n",
      "Epoch 97: accuracy did not improve from 0.99539\n",
      "1532/1532 [==============================] - 7s 5ms/step - loss: 0.1839 - accuracy: 0.9837\n",
      "Epoch 98/100\n",
      "1531/1532 [============================>.] - ETA: 0s - loss: 0.4207 - accuracy: 0.9661\n",
      "Epoch 98: accuracy did not improve from 0.99539\n",
      "1532/1532 [==============================] - 7s 5ms/step - loss: 0.4207 - accuracy: 0.9660\n",
      "Epoch 99/100\n",
      "1527/1532 [============================>.] - ETA: 0s - loss: 0.2135 - accuracy: 0.9786\n",
      "Epoch 99: accuracy did not improve from 0.99539\n",
      "1532/1532 [==============================] - 7s 5ms/step - loss: 0.2132 - accuracy: 0.9786\n",
      "Epoch 100/100\n",
      "1523/1532 [============================>.] - ETA: 0s - loss: 0.0154 - accuracy: 0.9948\n",
      "Epoch 100: accuracy did not improve from 0.99539\n",
      "1532/1532 [==============================] - 7s 5ms/step - loss: 0.0156 - accuracy: 0.9948\n"
     ]
    }
   ],
   "source": [
    "from keras import activations\n",
    "# #Building the LSTM Model\n",
    "lstm = Sequential()\n",
    "# unit = hidden state\n",
    "lstm.add(LSTM(units=64, input_shape=(time_steps, input_train_lstm.shape[2]), activation='relu', return_sequences=True))\n",
    "\n",
    "lstm.add(LSTM(units=128, activation='relu', return_sequences=True))\n",
    "\n",
    "lstm.add(LSTM(units=64, activation='relu', return_sequences=False))\n",
    "\n",
    "# lop dau vao hinh tron\n",
    "lstm.add(Dense(1)) \n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "lstm.compile(loss='binary_crossentropy', optimizer=opt, metrics='accuracy')\n",
    "lstm.summary()\n",
    "file_name = file_name + \".hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_name, monitor='accuracy', save_best_only=True, mode='auto', period=1, verbose=1)\n",
    "# early = EarlyStopping(monitor='accuracy')\n",
    "epoch=100\n",
    "history = lstm.fit(input_train_lstm,\n",
    "                   train_y,\n",
    "                   epochs=epoch,\n",
    "                   verbose=1,\n",
    "                   callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('new_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a66f67bcd2d9379639d3680a0af15e5300470e4e513dc74ad96ff4c58bd935f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
